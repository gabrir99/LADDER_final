#!python
# -*- coding: utf-8 -*-
# @author: Kun

"""
Author: Kun
Date: 2021-09-16 11:12:03
LastEditTime: 2021-09-23 18:02:44
LastEditors: Kun
Description: 
FilePath: /ThreatReportExtractor/main.py
"""

import os
import signal
import argparse
import re
from nltk import sent_tokenize
import spacy

from nlp_extractor.tokenizer import ThreatTokenizer

from preprocessings import PreProcessor
from role_generator import RoleGenerator

from graph_generator import GraphGenerator

from data_loader.pattern_loader import load_lists
from project_config import SEC_PATTERNS_FILE_PATH


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--asterisk", type=str, default="true", help="asterisk task (true, false)"
    )
    parser.add_argument(
        "--crf", type=str, default="true", help="crf task (true, false)"
    )
    parser.add_argument(
        "--rmdup", type=str, default="true", help="remove duplicate task (true, false)"
    )
    parser.add_argument(
        "--elip", type=str, default="false", help="ellipsis resolution (true, false)"
    )
    parser.add_argument("--gname", type=str, default="graph", help="graph name")
    parser.add_argument("--input_file", type=str, help="input file")
    args = parser.parse_args()
    print(args)

    ########################################################################################

    nlp = spacy.load("en_core_web_lg")

    titles_list = load_lists(SEC_PATTERNS_FILE_PATH)["MS_TITLES"]
    # strips remove combination of characters thus ], [ from beggining and end of the string.
    titles_list = titles_list.replace("'", "").strip("][").split(", ")

    main_verbs = load_lists(SEC_PATTERNS_FILE_PATH)["verbs"]
    main_verbs = main_verbs.replace("'", "").strip("][").split(", ")

    threat_tokenizer = ThreatTokenizer(nlp, main_verbs, titles_list)

    if not args.input_file:
        raise ValueError(
            "usage: main.py [-h] [--asterisk ASTERISK] [--crf CRF] [--rmdup RMDUP] [--gname GNAME] [--input_file INPUT_FILE]"
        )
    else:
        with open(args.input_file, encoding="iso-8859-1") as f:
            txt = f.readlines()
            txt = " ".join(txt)
            txt = txt.replace("\n", " ")

    pre_processor = PreProcessor(nlp)

    role_gen = RoleGenerator(nlp, main_verbs)

    graph_gen = GraphGenerator()

    ########################################################################################
    # TODO moved from tokenizer
    txt = threat_tokenizer.delete_brackets(txt)
    txt = txt.strip(" ")

    all_sentences_list = threat_tokenizer.removable_token(txt)

    txt_tokenized = threat_tokenizer.sentence_tokenizer(all_sentences_list)
    print(
        "*****sentence_tokenizer:",
        len(sent_tokenize(txt_tokenized)),
        threat_tokenizer.sentence_tokenizer(all_sentences_list),
    )

    print("*****Tokenizer*****")

    for i, val in enumerate(sent_tokenize(txt_tokenized)):
        print(i, val)

    ########################################################################################
    # TODO moved from preprocessings
    def SIGSEGV_signal_arises(signalNum, stack):
        print(f"{signalNum} : SIGSEGV arises")

    signal.signal(signal.SIGSEGV, SIGSEGV_signal_arises)

    # print("[spacy] begin load en_core_web_lg")
    # nlp = spacy.load("en_core_web_lg")
    preprocessing_input = threat_tokenizer.sentence_tokenizer(all_sentences_list)

    print("------------communicate ---------------")

    txt = preprocessing_input
    txt = pre_processor.delete_brackets(txt)

    txt = pre_processor.pass2acti(txt)
    txt = re.sub(" +", " ", txt)  # replace one or more spaces with a single space.
    print("*********8", txt)

    if args.crf == "true":
        print("args.crf is 'true'")
        print(txt)
        txt = pre_processor.coref_(txt)
        print("coref_", len(txt), txt)
    else:
        txt = pre_processor.wild_card_extansions(txt)

    txt = pre_processor.try_to(txt)
    print("try_to__", txt)
    txt = pre_processor.is_capable_of(txt)

    if args.elip == "true":
        txt = pre_processor.replcae_surrounding_subject(txt)
    else:
        print("is capble of__", txt)
        txt = pre_processor.ellipsis_subject(txt)
        print("ellipsis_subject", len(txt), txt)

    print("------------ coref_the_following_colon ------------")
    out = pre_processor.coref_the_following_colon(txt)

    for i, val in enumerate(sent_tokenize(out)):
        print(i, val)

    print("------------ coref_the_following_middle ------------")

    midle = pre_processor.coref_the_following_middle(out)

    for i, val in enumerate(sent_tokenize(midle)):
        print(i, val)

    out_translate = pre_processor.translate_obscure_words(out)
    print("*****homogenization:", pre_processor.homogenization(out_translate))
    homo = pre_processor.homogenization(out_translate)
    comm = pre_processor.communicate_to_sr(homo)
    print(comm)
    cc = pre_processor.CÙ€C(comm)

    print("------------ modification ---------------")

    print("----Preprocessed:----")
    for i, val in enumerate(sent_tokenize(pre_processor.modification_(cc))):
        print(i, val)

    ########################################################################################
    # TODO removed from role_generator
    # main_verbs = load_lists(SEC_PATTERNS_FILE_PATH)['verbs']
    # main_verbs = main_verbs.replace("'", "").strip('][').split(', ')

    ########################################################################################

    txt = pre_processor.modification_(cc)
    txt = txt.strip()
    txt = role_gen.colon_seprator_multiplication(txt)

    txt = re.sub(" +", " ", txt)
    sentences_ = sent_tokenize(txt)

    # TODO Role Generator
    lst = role_gen.roles(sentences_)
    lst = role_gen.fix_srl_spacing(lst)
    all_nodes = role_gen.negation_clauses(lst)
    if args.asterisk == "true":
        all_nodes = role_gen.astriks(all_nodes)
        all_nodes = role_gen.triplet_builder(all_nodes)
    else:
        all_nodes = role_gen.triplet_builder(all_nodes)

    ########################################################################################

    all_nodes = graph_gen.remove_no_sub(all_nodes)
    lst = graph_gen.remove_c_colon_toprevent_graphvizbug(all_nodes)
    for i in lst:
        if "\\'" in i:
            i.replace("\\'", "'")

    if args.rmdup == "true":
        lst = graph_gen.rm_duplictes(lst)
        graph_gen.graph_builder(lst, args.gname)
    else:
        graph_gen.graph_builder(lst, args.gname)
